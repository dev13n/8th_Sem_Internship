# 8th Semester Internship at Linde - IT Department (December 16, 2024 to April 15, 2025)

Welcome to my internship repository! I'm currently working as an intern in the IT department at **Linde**, a global leader in industrial gases and engineering. This repository will document my journey and daily tasks during my internship from **December 16, 2024**, to **April 15, 2025**. 

## Daily Tasks

### Week 1: Introduction to Linde and IT Department 🏢

#### Day 1: Monday, 16/12/2024
- **Introduction to Linde**: I was introduced to the company, its values, and its global impact in the industrial gases sector 🌍.
- **Team Introduction**: Met the members of the IT department and the fellow intern I'll be collaborating with. We received an overview of the team's role in supporting Linde's global operations. Additionally, we discussed our technical strengths, current projects, and potential ideas that could be developed into projects for us to work on. 💻.
- **HR Setup**: Completed essential HR-related tasks, including onboarding, and receiving necessary work materials such as ID card and gate pass 📝.
- **Campus Tour**: A guided tour around the Linde campus to familiarize myself with the workspace and facilities 🏫.

#### Day 2: Tuesday, 17/12/2024
- **Tuesday Toolbox Meeting**: Attended the morning "Tuesday Toolbox" meeting, which lasted for 15 minutes. We discussed Linde's company culture and explored ways to implement these values within our daily work life and projects 🤝.
- **Linde Credentials**: Received our official Linde credentials, which included login access and necessary documentation to get started with our tasks 🔑.
- **Familiarization with Projects**: Got introduced to the current projects within the IT department, including software tools and system optimizations ⚙️.
- **Initial Task Assignment**: Assigned to assist with [specific project or task] 🚀.

#### Day 3: Wednesday, 18/12/2024
- **Project Discussion with Supervisor**: Had a detailed discussion with our supervisor to redefine the project ideas, identifying potential challenges and brainstorming initial solutions for addressing them 🧠.
- **Dataset Research**: Conducted research to find relevant datasets that can be leveraged for the project, exploring various sources and evaluating their suitability 📊.
- **Data Engineering Solutions**: Provided data engineering solutions for certain aspects of the project, helping to streamline the data pipeline and optimize processes 🛠️.

#### Day 4: Thursday, 19/12/2024  
- **Python Libraries and Dependencies**: Created a detailed list of Python libraries and their specific versions to be used in the project, explaining the rationale behind choosing certain versions for compatibility and stability. Included libraries such as NumPy, Pandas, Scikit-learn, TensorFlow, and Matplotlib for data analysis, machine learning, and visualization 📚.
- **Temporary Plan of Action**: Outlined a preliminary action plan for the next stages of the project, defining clear tasks and milestones for upcoming days. The plan focuses on setting up the environment, data preprocessing, and model selection 🔧.
- **ML Techniques**: Identified machine learning techniques that could be useful for the project, such as supervised learning (regression, classification), unsupervised learning (clustering), and deep learning (neural networks) for more advanced modeling. Discussed how these techniques can be applied to the project's goals 💡.

#### Day 5: Friday, 20/12/2024 
- **Environment Setup**: Successfully installed the chosen IDE (PyCharm) 🎯 after exploring capabilities of other IDEs like GitHub Codespaces for remote development 🌐 and VSCode for its powerful extensions 💡. Set up a virtual environment using Anaconda 🐍 to isolate project dependencies and avoid conflicts.  
- **Dependency Exploration**: Investigated and compared different versions of key libraries (NumPy, Pandas, Scikit-learn, TensorFlow, Matplotlib) 📚 for optimal performance and compatibility. Explored alternative libraries for specific tasks, such as PDF processing libraries (e.g., PyPDF2, pdfminer) 📄 to determine the most suitable choices.  

#### Day 6: Saturday, 21/12/2024   
- **Weekend**: Relax 🌴 and recharge 🔋.  

#### Day 7: Sunday, 22/12/2024   
- **Weekend**: Enjoy leisure time 🌞🎉.

### Week 2: Worked on Named Entity Relation 🏢

#### Day 8: Monday, 23/12/2024   
- **Learning Doctr**: Explored **Doctr** 🛠️, an OCR (Optical Character Recognition) tool designed for extracting textual information from images and PDFs💻. It leverages deep learning models for accurate text recognition and layout analysis 📄. Gained an understanding of its key features, including:  
  - **Text Detection**: Identifies and localizes text in images or documents.  
  - **Text Recognition**: Converts the detected text into editable and searchable formats.  
  - **Structured Outputs**: Preserves layouts, such as tables or columns, ensuring organized data extraction 📋.  
- **Model Experimentation**: Ran the model on temporary finance-based images 💰 and mechanical tender-related PDFs 🛠️ to evaluate its performance. Analyzed the output, which was in JSON format, ensuring proper structure and usability for downstream tasks 📊.

#### Day 9: Tuesday, 24/12/2025  

- **Demo Presentation**: Showcased a demonstration of **Doctr** to the mentor, highlighting its capabilities in:  
  - **Text Extraction**: Accurately extracting text from finance-based images 💰 and tender-related PDFs 🛠️.  
  - **Structured Output**: Emphasized the well-organized JSON output, which is ideal for integration into data pipelines 📊.  
  - **Layout Preservation**: Demonstrated how Doctr retains document formatting, such as tables and multi-column text, ensuring usability in real-world applications 📄.  

- **Feedback and Dataset Sharing**:  
  - Mentor provided constructive feedback on enhancing the text extraction process for specific domains (e.g., finance and engineering).  
  - Received a curated dataset tailored for domain-specific text recognition and layout analysis 📂.  

#### Day 10: Wednesday, 25/12/2025  

- **Christmas Holiday 🎄**: Took a well-deserved break to celebrate Christmas with family and friends. Enjoyed the festivities, exchanged gifts, and recharged for upcoming tasks. 🎅✨

#### Day 11: Thursday, 26/12/2025  

- **Exploring spaCy**: Began integrating **spaCy**, a robust NLP library, into the workflow for further text analysis and processing:  
  - Set up pipelines for entity recognition and pattern matching.  
  - Tried all versions of spaCy models (**small**, **medium**, **large**, and **transformer-based**) to assess their performance:  
    - **Small**: Fast but limited in recognizing complex patterns.  
    - **Medium**: Balanced performance with improved recognition accuracy.  
    - **Large**: More accurate but slower, particularly for larger datasets.  
    - **Transformer-based**: Delivered the highest accuracy for complex texts but required significant computational resources.  
  - Focused on extracting structured data, such as names, dates, and monetary values, from financial documents 💰.  
  - Identified potential synergies between **Doctr** and **spaCy** for enhanced data extraction and organization.  

- **Table Extraction**: Initiated efforts to handle tables in documents more effectively:  
  - Used **Doctr's layout preservation** to detect table structures.  
  - Parsed and cleaned table data for seamless integration into analysis pipelines 📊.  

- **Data Engineering Thoughts**:  
  - Discussed ongoing plans for creating a robust data pipeline to support OCR and NLP workflows.  
  - Explored strategies for preprocessing incoming datasets, such as removing noise, handling missing values, and ensuring uniform formatting.  
  - Considered storage solutions for maintaining extracted and processed data for efficient retrieval and analysis.  

#### Day 11: Thursday, 26/12/2025  

- **Exploring spaCy**: Began integrating **spaCy**, a robust NLP library, into the workflow for further text analysis and processing:  
  - Set up pipelines for entity recognition and pattern matching 🛠️.  
  - Tried all versions of spaCy models (**small**, **medium**, **large**, and **transformer-based**) to assess their performance:  
    - **Small**: Fast ⚡ but limited in recognizing complex patterns.  
    - **Medium**: Balanced performance with improved recognition accuracy ⚙️.  
    - **Large**: More accurate but slower 🐢, particularly for larger datasets.  
    - **Transformer-based**: Delivered the highest accuracy 🌟 for complex texts but required significant computational resources 💾.  
  - Focused on extracting structured data, such as names, dates, and monetary values, from financial documents 💰📄.  
  - Identified potential synergies between **Doctr** and **spaCy** for enhanced data extraction and organization 🤝.  

- **Table Extraction**: Initiated efforts to handle tables in documents more effectively 📊:  
  - Used **Doctr's layout preservation** to detect table structures 🗂️.  
  - Parsed and cleaned table data for seamless integration into analysis pipelines ✅.  

- **Data Engineering Thoughts**:  
  - Discussed ongoing plans for creating a robust data pipeline to support OCR and NLP workflows 🌐.  
  - Explored strategies for preprocessing incoming datasets, such as removing noise 🧹, handling missing values 🧐, and ensuring uniform formatting 📏.  
  - Considered storage solutions 🛢️ for maintaining extracted and processed data for efficient retrieval and analysis 🔍.  

#### Day 12: Friday, 27/12/2025  

- **GenAI APIs Exploration**: Received inputs to test and integrate **Generative AI APIs** for text extraction and analysis tasks 🚀:  
  - Researched various APIs like OpenAI, Hugging Face, and Google Vertex AI 🔍.  
  - Conducted initial tests to evaluate their capabilities in handling complex documents, including dense text, tables, and mixed layouts 🛠️📄.  
  - Compared the outputs against existing tools (**Doctr** + **spaCy**) for performance benchmarking and identifying gaps 📊📈.  

- **Data Engineering Alignment**:  
  - Reflected on how the inclusion of Generative AI APIs can streamline workflows within the data pipeline 🤖.  
  - Brainstormed on how to manage large-scale outputs, ensuring seamless integration with preprocessing, storage, and visualization components 🧩.  
  - Identified the need for metadata tagging 🏷️ for processed documents to enhance traceability and usability 🚦.  

- **Next Steps**: Plan to refine workflows by combining **Doctr**, **spaCy**, and the most promising GenAI API into a comprehensive text extraction and processing pipeline while aligning with broader data engineering goals 🔄📌.  

22
